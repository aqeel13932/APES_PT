{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('filesignature',type=int)\n",
    "parser.add_argument('--batch_size', type=int, default=10)#100 ( 100, 16,32,64,128) priority 3\n",
    "parser.add_argument('--seed',type=int,default=1337)#4(CH)9(JAP)17(ITAL)\n",
    "parser.add_argument('--hidden_size', type=int, default=100)#priority 2\n",
    "parser.add_argument('--layers', type=int, default=1) #priority : 1.9 (it should learn regardless , but the quality differ ) \n",
    "parser.add_argument('--batch_norm', action=\"store_true\", default=False)#priority 5 , keep turned off\n",
    "parser.add_argument('--no_batch_norm', action=\"store_false\", dest='batch_norm')\n",
    "parser.add_argument('--replay_size', type=int, default=100000)# try increasing later  , priority 3.1\n",
    "parser.add_argument('--train_repeat', type=int, default=1)#(2^2) , priority 1\n",
    "parser.add_argument('--gamma', type=float, default=0.99)# (calculated should be 0.99) (0.99)\n",
    "parser.add_argument('--tau', type=float, default=0.001)# priority 0.9 (0.001 , 0.01 , 0.1) the one that work expeirment in the domain.\n",
    "parser.add_argument('--totalsteps', type=int, default=1000000)# much more ( 1000 -> 10,000) (should be around 1 million steps)\n",
    "parser.add_argument('--max_timesteps', type=int, default=1000)# 1000 \n",
    "parser.add_argument('--activation', choices=['tanh', 'relu'], default='relu')# experiment ( relu , tanh) priority 0.7\n",
    "parser.add_argument('--optimizer', choices=['adam', 'rmsprop'], default='adam')# priority 4.9\n",
    "#parser.add_argument('--optimizer_lr', type=float, default=0.001)#could be used later priority 4.5\n",
    "parser.add_argument('--exploration', type=float, default=0.1)# priority (0.8) it should decrease over time to reach 0.001 or even 0\n",
    "parser.add_argument('--vanish',type=float,default=0.75)#Decide when the exploration should stop in percentage (75%)\n",
    "parser.add_argument('--advantage', choices=['naive', 'max', 'avg'], default='avg')# priority 2 maybe done once and stike with one \n",
    "parser.add_argument('--rwrdschem',nargs='+',default=[-10,1000,-0.1],type=float) #(calculated should be (1000 reward , -0.1 punish per step)\n",
    "parser.add_argument('--svision',type=int,default=180)\n",
    "parser.add_argument('--details',type=str,default='')\n",
    "parser.add_argument('--train_m',type=str,default='')\n",
    "parser.add_argument('--target_m',type=str,default='')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "File_Signature=5000\n",
    "batch_size=32\n",
    "seed=1337\n",
    "hidden_size=100\n",
    "layers=1\n",
    "batch_norm=False\n",
    "replay_size=100000\n",
    "train_repeat=1\n",
    "gammma=0.99\n",
    "tau=0.001\n",
    "totalsteps=100\n",
    "max_timesteps=1000\n",
    "activation='tanh'\n",
    "optimizer='adam'\n",
    "#parser.add_argument('--optimizer_lr', type=float, default=0.001)#could be used later priority 4.5\n",
    "exploration=0.1\n",
    "vanish=0.75\n",
    "advantage='avg'\n",
    "rwrdschem=[-10,1000,-0.1]\n",
    "svision=180\n",
    "details='trying with tesnorflow'\n",
    "train_m=''\n",
    "target_m=''\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import skvideo.io\n",
    "from PD_Map import DPMP\n",
    "from Settings import *\n",
    "from World import *\n",
    "from Agent import *\n",
    "from Obstacles import *\n",
    "from Foods import *\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from buffer import Buffer\n",
    "import os\n",
    "from keras.layers import Input,Dense,Lambda\n",
    "from keras.models import Model,K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taken: 0.2096107006072998\n",
      "train default\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 613)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           61400       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 6)             606         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 5)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "test from scractch\n",
      "Episode 1 finished after 6 timesteps, episode reward -0.6 Tooks 1.1904888153076172s, Total Progress:6\n",
      "Episode 2 finished after 5 timesteps, episode reward -0.5 Tooks 0.03430461883544922s, Total Progress:11\n",
      "Episode 3 finished after 18 timesteps, episode reward -1.8000000000000005 Tooks 0.12624716758728027s, Total Progress:29\n",
      "Episode 4 finished after 14 timesteps, episode reward -1.4000000000000001 Tooks 0.10997819900512695s, Total Progress:43\n",
      "Episode 5 finished after 8 timesteps, episode reward -0.7999999999999999 Tooks 0.07803535461425781s, Total Progress:51\n",
      "Episode 6 finished after 13 timesteps, episode reward -1.3 Tooks 0.08938884735107422s, Total Progress:64\n",
      "Episode 7 finished after 5 timesteps, episode reward -0.5 Tooks 0.0358273983001709s, Total Progress:69\n",
      "Episode 8 finished after 12 timesteps, episode reward -1.2 Tooks 0.09991955757141113s, Total Progress:81\n",
      "Episode 9 finished after 19 timesteps, episode reward -1.9000000000000006 Tooks 0.13670921325683594s, Total Progress:100\n",
      "Testing Target Model\n",
      "Testing Episode 9 finished after 14 timesteps, episode reward -1.5000000000000002 Tooks 0.24763870239257812s, Total Progress:100\n"
     ]
    }
   ],
   "source": [
    "def SetupEnvironment():\n",
    "    Start = time()\n",
    "\n",
    "    #Add Pictures\n",
    "    Settings.SetBlockSize(20)\n",
    "    Settings.AddImage('Wall','Pics/wall.jpg')\n",
    "    Settings.AddImage('Food','Pics/food.jpg')\n",
    "\n",
    "    #Specify World Size\n",
    "    Settings.WorldSize=(11,11)\n",
    "\n",
    "    #Create Probabilities\n",
    "    obs = np.zeros(Settings.WorldSize)\n",
    "    ragnt = np.zeros(Settings.WorldSize)\n",
    "    gagnt = np.zeros(Settings.WorldSize)\n",
    "    food = np.zeros(Settings.WorldSize)\n",
    "    obs[3:8,5] = 1\n",
    "    ragnt[:,0] =1\n",
    "    gagnt[:,10]=1\n",
    "    food[:,4:7]=1\n",
    "    food[3:8,5] = 0\n",
    "\n",
    "    #Add Probabilities to Settings\n",
    "    Settings.AddProbabilityDistribution('Obs',obs)\n",
    "    Settings.AddProbabilityDistribution('ragnt',ragnt)\n",
    "    Settings.AddProbabilityDistribution('gagnt',gagnt)\n",
    "    Settings.AddProbabilityDistribution('food',food)\n",
    "\n",
    "    #Create World Elements\n",
    "    obs = Obstacles('Wall',Shape=np.array([[1],[1],[1],[1]]),PdstName='Obs')\n",
    "    food = Foods('Food',PdstName='food')\n",
    "\n",
    "    ragnt = Agent(Fname='Pics/ragent.jpg',Power=3,VisionAngle=svision,Range=-1,PdstName='ragnt')\n",
    "    gagnt = Agent(Fname='Pics/gagent.jpg',VisionAngle=180,Range=-1,Power=10,ControlRange=2,PdstName='gagnt')\n",
    "\n",
    "    game =World(RewardsScheme=rwrdschem,StepsLimit=max_timesteps)\n",
    "    #Adding Agents in Order of Following the action\n",
    "    game.AddAgents([ragnt,gagnt])\n",
    "    game.AddObstacles([obs])\n",
    "    game.AddFoods([food])\n",
    "    Start = time()-Start\n",
    "    print ('Taken:',Start)\n",
    "    return game\n",
    "\n",
    "def createLayers(insize,naction):\n",
    "    x = Input(shape=insize)#env.observation_space.shape)\n",
    "    if batch_norm:\n",
    "      h = BatchNormalization()(x)\n",
    "    else:\n",
    "      h = x\n",
    "    for i in range(layers):\n",
    "      h = Dense(hidden_size, activation=activation)(h)\n",
    "      if batch_norm and i != layers - 1:\n",
    "        h = BatchNormalization()(h)\n",
    "    y = Dense(naction + 1)(h)\n",
    "    if advantage == 'avg':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:] - K.mean(a[:, 1:], keepdims=True), output_shape=(naction,))(y)\n",
    "    elif advantage == 'max':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:] - K.max(a[:, 1:], keepdims=True), output_shape=(naction,))(y)\n",
    "    elif advantage == 'naive':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:], output_shape=(naction,))(y)\n",
    "    else:\n",
    "      assert False\n",
    "\n",
    "    return x, z\n",
    "TestingCounter=0\n",
    "def TryModel(model,game):\n",
    "    print('Testing Target Model')\n",
    "    global AIAgent,File_Signature,TestingCounter,DAgent\n",
    "    TestingCounter+=1\n",
    "    writer = skvideo.io.FFmpegWriter(\"output/{}/VID/{}_Test.avi\".format(File_Signature,TestingCounter))\n",
    "    #writer2 = skvideo.io.FFmpegWriter(\"output/{}/VID/{}_TestAG.avi\".format(File_Signature,TestingCounter))\n",
    "    game.GenerateWorld()\n",
    "    game.Step()\n",
    "    img = game.BuildImage()\n",
    "    Start = time()\n",
    "    episode_reward=0\n",
    "    observation = AIAgent.Flateoutput()\n",
    "\n",
    "    writer.writeFrame(np.array(img*255,dtype=np.uint8))\n",
    "    for t in range(max_timesteps):\n",
    "        s =np.array([observation])\n",
    "        q = model.predict(s, batch_size=1)\n",
    "        action = np.argmax(q[0])\n",
    "        AIAgent.NextAction = Settings.PossibleActions[action]\n",
    "        DAgent.DetectAndAstar()\n",
    "        game.Step()\n",
    "        writer.writeFrame(np.array(game.BuildImage()*255,dtype=np.uint8))\n",
    "        #writer2.writeFrame(np.array(game.AgentViewPoint(AIAgent.ID)*255,dtype=np.uint8))\n",
    "        observation = AIAgent.Flateoutput()\n",
    "        reward = AIAgent.CurrentReward\n",
    "        done = game.Terminated[0]\n",
    "\n",
    "        #observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        #print \"reward:\", reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    #writer2.close()\n",
    "    if t>=999:\n",
    "        plt.imsave('output/{}/PNG/{}_Test.png'.format(File_Signature,TestingCounter),img)\n",
    "    else:\n",
    "        os.remove(\"output/{}/VID/{}_Test.avi\".format(File_Signature,TestingCounter))\n",
    "        #os.remove(\"output/{}/VID/{}_TestAG.avi\".format(File_Signature,TestingCounter))\n",
    "\n",
    "    Start = time()-Start\n",
    "    print(\"Testing Episode {} finished after {} timesteps, episode reward {} Tooks {}s, Total Progress:{}\".format(i_episode, t, episode_reward,Start,progress))\n",
    "\n",
    "game = SetupEnvironment()\n",
    "AIAgent = game.agents[1001]\n",
    "DAgent = game.agents[1002]\n",
    "'''\n",
    "input size :\n",
    "Worldsize*(Agents Count+3)+Agents Count *4\n",
    "worldsize*(Agents count +3(food,observed,obstacles)) + Agents count *4 (orintation per agent)\n",
    "'''\n",
    "#ishape =(Settings.WorldSize[0]*Settings.WorldSize[1]*(len(game.agents)+3)+ len(game.agents)*4,)\n",
    "ishape =(Settings.WorldSize[0]*Settings.WorldSize[1]*(2+3)+ 2*4,)\n",
    "game.GenerateWorld()\n",
    "game.Step()\n",
    "naction =  Settings.PossibleActions.shape[0]\n",
    "\n",
    "if train_m=='':\n",
    "    print('train default')\n",
    "    x, z = createLayers(ishape,naction)\n",
    "    model = Model(input=x, output=z)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "else:\n",
    "    model = load_model('cur_mod/{}/model.h5'.format(train_m))\n",
    "if target_m=='':\n",
    "    print('test from scractch')\n",
    "    x, z = createLayers(ishape,naction)\n",
    "    target_model = Model(input=x, output=z)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "else:\n",
    "    target_model = load_model('cur_mod/{}/target_model.h5'.format(target_m))\n",
    "\n",
    "mem = Buffer(replay_size,ishape,(1,))\n",
    "#Exploration decrease amount:\n",
    "EDA = exploration/(totalsteps*vanish)\n",
    "#Framse Size\n",
    "fs = (Settings.WorldSize[0]*Settings.BlockSize[0],Settings.WorldSize[1]*Settings.BlockSize[1])\n",
    "total_reward = 0\n",
    "#Create Folder to store the output\n",
    "if not os.path.exists('output/{}'.format(File_Signature)):\n",
    "        os.makedirs('output/{}'.format(File_Signature))\n",
    "        os.makedirs('output/{}/PNG'.format(File_Signature))\n",
    "        os.makedirs('output/{}/VID'.format(File_Signature))\n",
    "        os.makedirs('output/{}/MOD'.format(File_Signature))\n",
    "        \n",
    "progress=0\n",
    "i_episode=0\n",
    "while progress<totalsteps:\n",
    "    i_episode+=1\n",
    "    game.GenerateWorld()\n",
    "\n",
    "    Start = time()\n",
    "    #First Step only do the calculation of the current observations for all agents\n",
    "    game.Step()\n",
    "    #Recording Video\n",
    "    img =game.BuildImage()\n",
    "    episode_reward=0\n",
    "    observation = AIAgent.Flateoutput()\n",
    "\n",
    "    for t in range(max_timesteps):\n",
    "        exploration = exploration-EDA\n",
    "        if np.random.random() < exploration:\n",
    "          action =AIAgent.RandomAction()\n",
    "        else:\n",
    "          s =np.array([observation])\n",
    "          q = model.predict(s, batch_size=1)\n",
    "          action = np.argmax(q[0])\n",
    "        prev_ob = observation\n",
    "        AIAgent.NextAction = Settings.PossibleActions[action]\n",
    "        DAgent.DetectAndAstar()\n",
    "        game.Step()\n",
    "        observation = AIAgent.Flateoutput()\n",
    "        reward = AIAgent.CurrentReward\n",
    "        done = game.Terminated[0]\n",
    "        #observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        #print \"reward:\", reward\n",
    "        mem.add(prev_ob,np.array([action]),reward,observation,done)\n",
    "        for k in range(train_repeat):\n",
    "            prestates,actions,rewards,poststates,terminals = mem.sample(batch_size)\n",
    "\n",
    "            qpre = model.predict(prestates)\n",
    "            qpost = model.predict(poststates)\n",
    "            for i in range(qpre.shape[0]):\n",
    "                if terminals[i]:\n",
    "                    qpre[i, actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    try:\n",
    "                        qpre[i, actions[i]] = rewards[i] + gammma * np.amax(qpost[i])\n",
    "                    except Exception as ex:\n",
    "                        print(ex)\n",
    "                        print('qpre.shape:{},i:{},actions:{}'.format(qpre.shape,i,actions[i]))\n",
    "            model.train_on_batch(prestates, qpre)\n",
    "            weights = model.get_weights()\n",
    "            target_weights = target_model.get_weights()\n",
    "            for i in range(len(weights)):\n",
    "                target_weights[i] = tau * weights[i] + (1 - tau) * target_weights[i]\n",
    "            target_model.set_weights(target_weights)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    if t>=999:\n",
    "\n",
    "        plt.imsave('output/{}/PNG/{}_train.png'.format(File_Signature,i_episode),img)\n",
    "    aiprob =0# DPMP(wmap,agindx,findx,t+1)\n",
    "    Start = time()-Start\n",
    "    t = t+1\n",
    "    progress+=t\n",
    "    \n",
    "    print(\"Episode {} finished after {} timesteps, episode reward {} Tooks {}s, Total Progress:{}\".format(i_episode, t, episode_reward,Start,progress))\n",
    "    total_reward += episode_reward\n",
    "    if i_episode%10==0:\n",
    "        TryModel(target_model,game)\n",
    "        print(\"Average reward per episode {}\".format(total_reward /i_episode))\n",
    "model.save('output/{}/MOD/model.h5'.format(File_Signature))\n",
    "target_model.save('output/{}/MOD/target_model.h5'.format(File_Signature))\n",
    "TryModel(target_model,game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "File_Signature=5000\n",
    "batch_size=32\n",
    "seed=1337\n",
    "hidden_size=100\n",
    "layers=1\n",
    "batch_norm=False\n",
    "replay_size=100000\n",
    "train_repeat=1\n",
    "gammma=0.99\n",
    "tau=0.001\n",
    "totalsteps=100\n",
    "max_timesteps=1000\n",
    "activation='tanh'\n",
    "optimizer='adam'\n",
    "#parser.add_argument('--optimizer_lr', type=float, default=0.001)#could be used later priority 4.5\n",
    "exploration=0.1\n",
    "vanish=0.75\n",
    "advantage='avg'\n",
    "rwrdschem=[-10,1000,-0.1]\n",
    "svision=180\n",
    "details='trying with tesnorflow'\n",
    "train_m=''\n",
    "target_m=''\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CreatetfNetwork(insize,naction):\n",
    "    inputl = tf.placeholder(tf.float32,shape=(None,insize))\n",
    "    print('input:',inputl.get_shape())\n",
    "    tf_W = tf.Variable(tf.truncated_normal((insize,hidden_size)))\n",
    "    tf_b = tf.Variable(tf.zeros(hidden_size))\n",
    "    tf_hidlayer= tf.tanh(tf.matmul(inputl,tf_W))\n",
    "    print('hidden-layer:',tf_hidlayer.get_shape())\n",
    "    tf_Adv,tf_v = tf.split(tf_hidlayer,2,axis=1)\n",
    "    print('advantage:{},Value:{}'.format(tf_Adv.get_shape(),tf_v.get_shape()))\n",
    "    tf_adv_w = tf.Variable(tf.truncated_normal((hidden_size//2,naction)))\n",
    "    tf_adv_b = tf.Variable(tf.zeros(naction))\n",
    "    tf_v_w = tf.Variable(tf.truncated_normal((hidden_size//2,1)))\n",
    "    tf_v_b = tf.Variable(tf.zeros(1))\n",
    "    tf_AdvL = tf.matmul(tf_Adv,tf_adv_w)+ tf_adv_b\n",
    "    tf_VL= tf.matmul(tf_v,tf_v_w)+ tf_v_b\n",
    "    if advantage == 'avg':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:] - K.mean(a[:, 1:], keepdims=True), output_shape=(naction,))(y)\n",
    "    elif advantage == 'max':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:] - K.max(a[:, 1:], keepdims=True), output_shape=(naction,))(y)\n",
    "    elif advantage == 'naive':\n",
    "        tf_final= tf\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:], output_shape=(naction,))(y)\n",
    "    else:\n",
    "      assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (?, 613)\n",
      "hidden-layer: (?, 100)\n",
      "advantage:(?, 50),Value:(?, 50)\n"
     ]
    }
   ],
   "source": [
    "CreatetfNetwork(613,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLayers(insize,naction):\n",
    "    x = Input(shape=insize)#env.observation_space.shape)\n",
    "    if batch_norm:\n",
    "      h = BatchNormalization()(x)\n",
    "    else:\n",
    "      h = x\n",
    "    for i in range(layers):\n",
    "      h = Dense(hidden_size, activation=activation)(h)\n",
    "      if batch_norm and i != layers - 1:\n",
    "        h = BatchNormalization()(h)\n",
    "    y = Dense(naction + 1)(h)\n",
    "    if advantage == 'avg':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:] - K.mean(a[:, 1:], keepdims=True), output_shape=(naction,))(y)\n",
    "    elif advantage == 'max':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:] - K.max(a[:, 1:], keepdims=True), output_shape=(naction,))(y)\n",
    "    elif advantage == 'naive':\n",
    "      z = Lambda(lambda a: K.expand_dims(a[:,0], dim=-1) + a[:,1:], output_shape=(naction,))(y)\n",
    "    else:\n",
    "      assert False\n",
    "\n",
    "    return x, z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
