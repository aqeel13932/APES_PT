{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2019/11/25 10:27 This file is the latest, you can launch ego centric and allocentric experiements from same file also you can launch all 3 levels.\n",
    "#2019/12/12 19:11 This file updated the size of allo-centric map to 13x13, removed myorientation layer from ego-centric map, adpated probability matrices to work with it.\n",
    "#2020/01/11 7:47 This file upate to remove any training trace. It'll be recording only.\n",
    "episodes=3\n",
    "rwrdschem=[-10,1000,-0.1]\n",
    "svision=180\n",
    "naction=0\n",
    "Ego=False\n",
    "Level=1\n",
    "import numpy as np\n",
    "import skvideo.io\n",
    "from APES import *\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 1002\n",
      "Taken: 2.2793757915496826\n",
      "ICD:1,ICF:1,DCF:1\n",
      "ICD:1,ICF:1,DCF:0\n",
      "ICD:1,ICF:1,DCF:0\n"
     ]
    }
   ],
   "source": [
    "def New_Reward_Function(agents,foods,rwrdschem,world,AES,Terminated):\n",
    "    \"\"\"Calculate All agents rewards\n",
    "    Args:\n",
    "        * agents: dictionary of agents contain all agents by ID\n",
    "        * foods: dictionary of all foods\n",
    "        * rwrdschem: Reward Schema (More info in World __init__)\n",
    "        * world: World Map\n",
    "        * AES: one element array\n",
    "    TODO:\n",
    "        * copy this function to class or __init__ documentation as example of how to build customer reward function\n",
    "        * Assign Reward To Agents\n",
    "        * Impelent the Food Reward Part depending on the decision of who take the food reward if two \n",
    "          agent exist in food range in same time\n",
    "        * Change All Ranges to .ControlRange not (-1) it's -1 only for testing purpuse\n",
    "        * Change Punish per step to not punish when agent do nothing\"\"\"\n",
    "    def ResetagentReward(ID):\n",
    "        #Punish for step \n",
    "        agents[ID].CurrentReward= rwrdschem[2] # -1 # rwrdschem[2] if len(agents[ID].NextAction)>0 else 0\n",
    "\n",
    "    for x in agents:\n",
    "        ResetagentReward(x)\n",
    "\n",
    "    AvailableFoods = world[(world>2000)&(world<=3000)]\n",
    "    if len(AvailableFoods)==0:\n",
    "        AES[0]-=1\n",
    "        Terminated[0]= True if AES[0]<=0 else Terminated[0]\n",
    "    for ID in agents.keys():\n",
    "        if agents[ID].IAteFoodID >-1:\n",
    "            agents[ID].CurrentReward+= foods[agents[ID].IAteFoodID].Energy* rwrdschem[1]\n",
    "        agntcenter = World._GetElementCoords(ID,agents[ID].FullEgoCentric)\n",
    "        aborder = World._GetVisionBorders(agntcenter,agents[ID].ControlRange,agents[ID].FullEgoCentric.shape)\n",
    "        \n",
    "def SetupEnvironment():\n",
    "    Start = time()\n",
    "    #Add Pictures\n",
    "    Settings.SetBlockSize(20)\n",
    "    Settings.AddImage('Wall','APES/Pics/wall.jpg')\n",
    "    Settings.AddImage('Food','APES/Pics/food.jpg')\n",
    "    #Specify World Size\n",
    "    if Ego: \n",
    "        Settings.WorldSize=(11,11)\n",
    "    #If the map is allocentric, we use larger word to compensate for the extra input in the ego-centric.\n",
    "    else:\n",
    "        Settings.WorldSize=(13,13)\n",
    "\n",
    "    #Create Probabilities\n",
    "    red_Ag_PM = np.zeros(Settings.WorldSize)\n",
    "    blue_Ag_PM = np.zeros(Settings.WorldSize)\n",
    "    food_PM = np.zeros(Settings.WorldSize)\n",
    "    blue_Ag_PM[:,0] =1\n",
    "    if Level==1:\n",
    "        if Ego:\n",
    "            red_Ag_PM[2,4]=1\n",
    "            food_PM[5,5] = 1\n",
    "        else:\n",
    "            red_Ag_PM[2,3]=1\n",
    "            food_PM[6,5] = 1\n",
    "    elif Level==2:\n",
    "        if Ego:\n",
    "            red_Ag_PM[5,5]=1\n",
    "            food_PM[3:8,3:8] = 1\n",
    "            food_PM[5,5]=0\n",
    "        else:\n",
    "            red_Ag_PM[6,5]=1\n",
    "            food_PM[4:9,3:8] = 1\n",
    "            food_PM[6,5]=0\n",
    "    elif Level==3:\n",
    "        if Ego:\n",
    "            red_Ag_PM[3:8,3:8]=1\n",
    "            food_PM[3:8,3:8] = 1\n",
    "        else:\n",
    "            red_Ag_PM[4:9,3:8]=1\n",
    "            food_PM[4:9,3:8] = 1\n",
    "            \n",
    "    #Add Probabilities to Settings\n",
    "    Settings.AddProbabilityDistribution('red_Ag_PM',red_Ag_PM)\n",
    "    Settings.AddProbabilityDistribution('blue_Ag_PM',blue_Ag_PM)\n",
    "    Settings.AddProbabilityDistribution('food_PM',food_PM)\n",
    "    #Create World Elements\n",
    "    food = Foods('Food',PdstName='food_PM')\n",
    "\n",
    "    blue_Ag = Agent(Fname='APES/Pics/blue.jpg',\n",
    "                    Power=3,\n",
    "                    VisionAngle=svision,Range=-1,\n",
    "                    PdstName='blue_Ag_PM',\n",
    "                    ActionMemory=naction,\n",
    "                   EgoCentric=Ego)\n",
    "    red_Ag = Agent(Fname='APES/Pics/red.jpg',\n",
    "                   VisionAngle=180,Range=-1,\n",
    "                   Power=10,\n",
    "                   ControlRange=1,\n",
    "                   PdstName='red_Ag_PM')\n",
    "    print(blue_Ag.ID,red_Ag.ID)\n",
    "    game=World(RewardsScheme=rwrdschem,StepsLimit=100,RewardFunction=New_Reward_Function)\n",
    "    #Agents added first has priority of executing there actions first.\n",
    "    #game.AddAgents([ragnt])\n",
    "    game.AddAgents([red_Ag,blue_Ag])\n",
    "    game.AddFoods([food])\n",
    "    Start = time()-Start\n",
    "    print ('Taken:',Start)\n",
    "    return game\n",
    "\n",
    "\n",
    "\n",
    "game = SetupEnvironment()\n",
    "\n",
    "AIAgent = game.agents[1001]\n",
    "DAgent = game.agents[1002]\n",
    "if Ego:\n",
    "    cnn =np.zeros((episodes,Settings.WorldSize[0],Settings.WorldSize[1]*2-1,3,),dtype=int8)\n",
    "    rest =np.zeros((episodes,naction*5+4,),dtype=int8)\n",
    "else:\n",
    "    cnn =np.zeros((episodes,Settings.WorldSize[0],Settings.WorldSize[1] ,4,),dtype=int8)\n",
    "    rest =np.zeros((episodes,naction*5+8,),dtype=int8)\n",
    "dom_see_food = np.zeros((episodes,1),dtype=int8)\n",
    "\n",
    "for i in range(episodes):\n",
    "    game.GenerateWorld()\n",
    "    AIAgent.Direction='E'\n",
    "    game.Step()\n",
    "    AIAgent.NNFeed['obstacles']=[]\n",
    "    I_C_DOM = game.agents[1001].NNFeed['agentori1002'].sum() #I see Dominante \n",
    "    I_C_FOOD = game.agents[1001].NNFeed['food'].sum() # I See Food\n",
    "    DOM_C_FOOD=game.agents[1002].NNFeed['food'].sum() # Dominant See Food.\n",
    "    metric = I_C_DOM and I_C_FOOD and DOM_C_FOOD\n",
    "    Start = time()\n",
    "    episode_reward=0\n",
    "    cnn[i],rest[i] = AIAgent.Convlutional_output()\n",
    "    dom_see_food[i] = metric\n",
    "    \n",
    "    \n",
    "    print('ICD:{},ICF:{},DCF:{}'.format(I_C_DOM,I_C_FOOD,DOM_C_FOOD))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 13, 13, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
